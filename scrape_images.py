#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Technopolis Ã¼rÃ¼n URL'lerinden gÃ¶rselleri Ã§eken ve Excel'e link olarak yazan script
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin, urlparse, urlunparse
import json
import re

# Ayarlar
EXCEL_FILE = 'Technopolis_Tum_Urunler_20250917_164841_Brands_Translated_NoDuplicates (1).xlsx'
DELAY = 1  # Her istek arasÄ±nda bekleme sÃ¼resi (saniye)
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def convert_to_full_size_image(img_url):
    """Thumbnail veya kÃ¼Ã§Ã¼k gÃ¶rsel URL'ini orijinal bÃ¼yÃ¼k gÃ¶rsele Ã§evirir"""
    if not img_url:
        return img_url
    
    # 71x71, 100x100 gibi kÃ¼Ã§Ã¼k boyutlarÄ± orijinal boyuta Ã§evir
    # Ã–nce yaygÄ±n pattern'leri kontrol et
    
    # Pattern 1: /71x71/, /100x100/ gibi kÃ¼Ã§Ã¼k boyutlarÄ± bÃ¼yÃ¼k boyutla deÄŸiÅŸtir
    size_pattern = r'/(\d+)x(\d+)/'
    match = re.search(size_pattern, img_url)
    if match:
        width, height = int(match.group(1)), int(match.group(2))
        # EÄŸer kÃ¼Ã§Ã¼k bir boyutsa (Ã¶rneÄŸin 71x71, 100x100), boyut kÄ±smÄ±nÄ± kaldÄ±r
        # (orijinal gÃ¶rsel URL'sini elde etmek iÃ§in)
        if width <= 200 or height <= 200:
            # Boyut kÄ±smÄ±nÄ± tamamen kaldÄ±r (orijinal boyut iÃ§in)
            img_url = re.sub(size_pattern, '/', img_url)
    
    # Pattern 2: thumb, thumbnail, small gibi kelimeler
    img_url = re.sub(r'/thumb(?:nail)?s?/', '/large/', img_url, flags=re.IGNORECASE)
    img_url = re.sub(r'/small/', '/large/', img_url, flags=re.IGNORECASE)
    img_url = re.sub(r'thumb(?:nail)?', 'large', img_url, flags=re.IGNORECASE)
    img_url = re.sub(r'_small', '_large', img_url, flags=re.IGNORECASE)
    img_url = re.sub(r'_thumb', '_large', img_url, flags=re.IGNORECASE)
    
    # Pattern 3: Query parametrelerinde boyut varsa kaldÄ±r veya deÄŸiÅŸtir
    parsed = urlparse(img_url)
    if parsed.query:
        # width, height, size gibi parametreleri kaldÄ±r
        query_params = []
        for param in parsed.query.split('&'):
            if not any(key in param.lower() for key in ['width', 'height', 'size', 'w=', 'h=']):
                query_params.append(param)
        new_query = '&'.join(query_params)
        img_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, new_query, parsed.fragment))
    
    # Pattern 4: URL'de _71x71_ gibi pattern varsa
    img_url = re.sub(r'_\d+x\d+_', '_', img_url)
    
    # Pattern 5: URL sonunda ?w=71&h=71 gibi parametreler
    img_url = re.sub(r'[?&](?:w|width|h|height|size)=\d+', '', img_url)
    img_url = img_url.rstrip('&?')
    
    return img_url

def get_images_from_url(url):
    """Verilen URL'den Ã¼rÃ¼n gÃ¶rsellerini Ã§eker"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        images = []
        
        # FarklÄ± gÃ¶rsel kaynaklarÄ±nÄ± dene
        
        # 0. Ã–NCE: Modal/Gallery iÃ§in data attribute'larÄ±nÄ± kontrol et (bÃ¼yÃ¼k gÃ¶rseller iÃ§in)
        # data-zoom-url, data-large-url, data-full-url, data-original gibi attribute'larÄ± ara
        for img in soup.find_all(['img', 'a', 'div'], attrs=lambda x: x and any(key in x for key in ['data-zoom-url', 'data-large-url', 'data-full-url', 'data-original', 'data-zoom', 'data-lightbox', 'data-gallery-url', 'data-href'])):
            for attr in ['data-zoom-url', 'data-large-url', 'data-full-url', 'data-original', 'data-zoom', 'data-lightbox', 'data-gallery-url', 'data-href']:
                large_url = img.get(attr)
                if large_url:
                    images.append(large_url)
        
        # 0.1: Gallery modal iÃ§in Ã¶zel attribute'lar
        gallery_items = soup.find_all(attrs={'data-image': True}) + soup.find_all(attrs={'data-thumb': True})
        for item in gallery_items:
            if item.get('data-image'):
                images.append(item.get('data-image'))
            if item.get('data-thumb'):
                # data-thumb genelde kÃ¼Ã§Ã¼k gÃ¶rseldir, ama bazen data-image ile birlikte gelir
                pass
        
        # 1. img etiketlerini kontrol et (product images iÃ§in)
        product_images = soup.find_all('img', class_=lambda x: x and ('product' in x.lower() or 'gallery' in x.lower() or 'main' in x.lower()))
        
        # 2. picture source etiketlerini kontrol et
        picture_tags = soup.find_all('picture')
        for picture in picture_tags:
            source_tags = picture.find_all('source')
            img_tags = picture.find_all('img')
            for source in source_tags:
                if source.get('srcset'):
                    # srcset'teki tÃ¼m gÃ¶rselleri al, en bÃ¼yÃ¼k olanÄ± seÃ§
                    srcset_items = source.get('srcset').split(',')
                    # En bÃ¼yÃ¼k gÃ¶rseli bul (geniÅŸlik deÄŸerine gÃ¶re)
                    largest_url = None
                    largest_size = 0
                    for item in srcset_items:
                        parts = item.strip().split()
                        url_part = parts[0]
                        # Boyut bilgisini al (varsa)
                        size = 0
                        if len(parts) > 1:
                            try:
                                size = int(re.sub(r'[^0-9]', '', parts[1]))
                            except:
                                pass
                        if size > largest_size:
                            largest_size = size
                            largest_url = url_part
                    if largest_url:
                        images.append(largest_url)
                    else:
                        # Boyut bilgisi yoksa, tÃ¼m URL'leri ekle
                        images.extend([src.split()[0] for src in srcset_items])
            for img in img_tags:
                # img etiketlerinde srcset varsa, en bÃ¼yÃ¼k olanÄ± seÃ§
                if img.get('srcset'):
                    srcset_items = img.get('srcset').split(',')
                    largest_url = None
                    largest_size = 0
                    for item in srcset_items:
                        parts = item.strip().split()
                        url_part = parts[0]
                        size = 0
                        if len(parts) > 1:
                            try:
                                size = int(re.sub(r'[^0-9]', '', parts[1]))
                            except:
                                pass
                        if size > largest_size:
                            largest_size = size
                            largest_url = url_part
                    if largest_url:
                        images.append(largest_url)
                if img.get('src'):
                    images.append(img.get('src'))
                if img.get('data-src'):
                    images.append(img.get('data-src'))
        
        # 3. data-src veya lazy-loaded gÃ¶rseller
        lazy_images = soup.find_all('img', {'data-src': True})
        for img in lazy_images:
            images.append(img.get('data-src'))
        
        # 4. Genel img etiketleri (yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ olanlarÄ±)
        all_images = soup.find_all('img')
        for img in all_images:
            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if src and any(keyword in src.lower() for keyword in ['product', 'gallery', 'main', 'zoom', 'big', 'large']):
                if src not in images:
                    images.append(src)
        
        # 5. JavaScript'te embed edilmiÅŸ gÃ¶rseller (API Ã§aÄŸrÄ±larÄ± vb)
        # 5.1: application/json type script'ler - Technopolis Ã¶zel yapÄ±sÄ±
        scripts = soup.find_all('script', type='application/json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict):
                    # Technopolis Ã¶zel yapÄ±sÄ±: cx-state.product.details.entities.{productId}.variants.value.images.GALLERY
                    if 'cx-state' in data and 'product' in data['cx-state']:
                        product_state = data['cx-state'].get('product', {})
                        if 'details' in product_state and 'entities' in product_state['details']:
                            for product_id, product_data in product_state['details']['entities'].items():
                                try:
                                    if 'variants' in product_data and 'value' in product_data['variants']:
                                        variants = product_data['variants']['value']
                                        if 'images' in variants:
                                            # PRIMARY gÃ¶rseli (ana gÃ¶rsel)
                                            if 'PRIMARY' in variants['images']:
                                                primary = variants['images']['PRIMARY']
                                                if isinstance(primary, dict) and 'videoluxZoom' in primary:
                                                    zoom_url = primary['videoluxZoom'].get('url')
                                                    if zoom_url:
                                                        images.append(zoom_url)
                                            
                                            # GALLERY gÃ¶rselleri (diÄŸer gÃ¶rseller) - videoluxZoom formatÄ±nÄ± Ã¶ncelikli al
                                            if 'GALLERY' in variants['images']:
                                                gallery = variants['images']['GALLERY']
                                                if isinstance(gallery, list):
                                                    for gallery_item in gallery:
                                                        if isinstance(gallery_item, dict):
                                                            # Ã–nce videoluxZoom'u dene (en bÃ¼yÃ¼k boyut)
                                                            if 'videoluxZoom' in gallery_item:
                                                                zoom_url = gallery_item['videoluxZoom'].get('url')
                                                                if zoom_url:
                                                                    images.append(zoom_url)
                                                            # Fallback: videoluxProduct
                                                            elif 'videoluxProduct' in gallery_item:
                                                                prod_url = gallery_item['videoluxProduct'].get('url')
                                                                if prod_url:
                                                                    images.append(prod_url)
                                except:
                                    pass
                    
                    # Genel nested structure kontrolÃ¼ (fallback)
                    def extract_urls(obj, urls_list):
                        if isinstance(obj, dict):
                            for key, value in obj.items():
                                if 'image' in key.lower() or 'photo' in key.lower() or 'img' in key.lower() or 'media' in key.lower() or 'gallery' in key.lower():
                                    if isinstance(value, str) and (value.startswith('http') or value.startswith('//')):
                                        urls_list.append(value)
                                    elif isinstance(value, list):
                                        for item in value:
                                            if isinstance(item, str) and (item.startswith('http') or item.startswith('//')):
                                                urls_list.append(item)
                                extract_urls(value, urls_list)
                        elif isinstance(obj, list):
                            for item in obj:
                                extract_urls(item, urls_list)
                    
                    # EÄŸer Technopolis yapÄ±sÄ±nda gÃ¶rsel bulunamadÄ±ysa, genel arama yap
                    if not any('technopolis.bg' in img for img in images):
                        extract_urls(data, images)
            except:
                pass
        
        # 5.2: TÃ¼m script tag'lerinde product images array'lerini ara
        all_scripts = soup.find_all('script')
        for script in all_scripts:
            if not script.string:
                continue
            script_text = script.string
            
            # JavaScript object'lerinde product images array'lerini ara
            # Pattern: images: [...], productImages: [...], gallery: [...], media: [...]
            patterns = [
                r'(?:images|productImages|gallery|media|productMedia)\s*[:=]\s*\[(.*?)\]',
                r'(?:zoom|large|full)Images\s*[:=]\s*\[(.*?)\]',
            ]
            
            for pattern in patterns:
                matches = re.finditer(pattern, script_text, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    array_content = match.group(1)
                    # URL'leri Ã§Ä±kar
                    url_matches = re.findall(r'["\'](https?://[^"\']+)["\']', array_content)
                    images.extend(url_matches)
                    # GÃ¶receli URL'ler iÃ§in
                    rel_urls = re.findall(r'["\'](/[^"\']+\.(?:jpg|jpeg|png|webp|gif))["\']', array_content, re.IGNORECASE)
                    images.extend(rel_urls)
            
            # JSON.parse() iÃ§indeki verileri ara
            json_matches = re.finditer(r'JSON\.parse\(["\'](.*?)["\']\)', script_text, re.DOTALL)
            for json_match in json_matches:
                try:
                    json_str = json_match.group(1).replace('\\"', '"').replace("\\'", "'")
                    json_data = json.loads(json_str)
                    def extract_from_obj(obj):
                        if isinstance(obj, dict):
                            for key, val in obj.items():
                                if any(kw in key.lower() for kw in ['image', 'gallery', 'media', 'zoom', 'large']):
                                    if isinstance(val, str) and ('http' in val or val.startswith('/')):
                                        images.append(val)
                                    elif isinstance(val, list):
                                        for item in val:
                                            if isinstance(item, str) and ('http' in item or item.startswith('/')):
                                                images.append(item)
                                extract_from_obj(val)
                        elif isinstance(obj, list):
                            for item in obj:
                                extract_from_obj(item)
                    extract_from_obj(json_data)
                except:
                    pass
        
        # URL'leri normalize et
        normalized_images = []
        base_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}"
        
        for img_url in images:
            if not img_url:
                continue
            # GÃ¶receli URL'leri mutlak URL'lere Ã§evir
            if img_url.startswith('//'):
                img_url = f"{urlparse(url).scheme}:{img_url}"
            elif img_url.startswith('/'):
                img_url = urljoin(base_url, img_url)
            elif not img_url.startswith('http'):
                img_url = urljoin(url, img_url)
            
            # TekrarlarÄ± temizle ve geÃ§erli gÃ¶rselleri filtrele
            if img_url not in normalized_images and any(ext in img_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif']):
                normalized_images.append(img_url)
        
        # Logo, icon gibi gÃ¶rselleri filtrele ve kÃ¼Ã§Ã¼k gÃ¶rselleri bÃ¼yÃ¼k versiyonlarÄ±na Ã§evir
        filtered_images = []
        exclude_keywords = ['logo', 'icon', 'banner', 'placeholder', 'blank', 'no-image', 'social']
        
        for img_url in normalized_images:
            if not any(keyword in img_url.lower() for keyword in exclude_keywords):
                # videoluxZoom ve videoluxProduct URL'leri zaten bÃ¼yÃ¼k gÃ¶rseller, dÃ¶nÃ¼ÅŸtÃ¼rme yapma
                is_videolux_url = 'videoluxzoom' in img_url.lower() or 'videoluxproduct' in img_url.lower() or 'product-zoom' in img_url.lower()
                
                if is_videolux_url:
                    # Zaten bÃ¼yÃ¼k gÃ¶rsel, direkt ekle
                    full_size_url = img_url
                else:
                    # KÃ¼Ã§Ã¼k gÃ¶rselleri (thumbnail'ler) orijinal bÃ¼yÃ¼k versiyonlarÄ±na Ã§evir
                    full_size_url = convert_to_full_size_image(img_url)
                
                # videoluxZoom URL'lerini en Ã¶ncelikli yap (bÃ¼yÃ¼k gÃ¶rseller)
                if 'videoluxzoom' in img_url.lower() or 'product-zoom' in img_url.lower():
                    filtered_images.insert(0, full_size_url)
                # ÃœrÃ¼n gÃ¶rseli gibi gÃ¶rÃ¼nen URL'leri Ã¶nceliklendir
                elif any(keyword in img_url.lower() for keyword in ['product', 'gallery', 'main', 'zoom', 'big', 'large', '/p/', '/products/']):
                    filtered_images.insert(0, full_size_url)
                else:
                    filtered_images.append(full_size_url)
        
        # TekrarlarÄ± temizle (aynÄ± gÃ¶rselin farklÄ± boyutlarÄ± olabilir)
        unique_images = []
        seen = set()
        for img_url in filtered_images:
            # URL'yi normalize et (protocol, domain olmadan karÅŸÄ±laÅŸtÄ±r)
            normalized = urlparse(img_url).path.lower()
            if normalized not in seen:
                seen.add(normalized)
                unique_images.append(img_url)
        
        # En fazla 10 gÃ¶rsel dÃ¶ndÃ¼r
        return unique_images[:10]
        
    except Exception as e:
        print(f"  Hata: {str(e)}")
        return []

def main():
    print("Excel dosyasÄ± okunuyor...")
    df = pd.read_excel(EXCEL_FILE)
    
    # Product URL sÃ¼tununu kontrol et
    url_column = 'Product URL'
    if url_column not in df.columns:
        print(f"Hata: '{url_column}' sÃ¼tunu bulunamadÄ±!")
        print(f"Mevcut sÃ¼tunlar: {df.columns.tolist()}")
        return
    
    # Yeni sÃ¼tunlarÄ± ekle (eÄŸer yoksa)
    if 'Ana gÃ¶rsel' not in df.columns:
        df['Ana gÃ¶rsel'] = ''
    if 'DiÄŸer gÃ¶rseller' not in df.columns:
        df['DiÄŸer gÃ¶rseller'] = ''
    
    # BoÅŸ olmayan URL'leri filtrele
    df_with_urls = df[df[url_column].notna()].copy()
    total_products = len(df_with_urls)
    
    print(f"Toplam {total_products} Ã¼rÃ¼n bulundu.")
    print(f"GÃ¶rsel linkleri Excel'e yazÄ±lacak.\n")
    
    # Ä°lerleme iÃ§in stats
    stats = {
        'success': 0,
        'no_images': 0
    }
    
    # Her kaÃ§ Ã¼rÃ¼nde bir Excel'i kaydet (ilerlemeyi korumak iÃ§in)
    SAVE_INTERVAL = 10
    
    for idx, (index, row) in enumerate(df_with_urls.iterrows(), 1):
        product_id = row.get('Product ID', f'product_{index}')
        product_name = row.get('Product Name', 'Unknown')
        url = row[url_column]
        
        print(f"[{idx}/{total_products}] Product ID: {product_id}")
        print(f"  ÃœrÃ¼n: {product_name[:50]}...")
        print(f"  URL: {url}")
        
        # GÃ¶rselleri Ã§ek
        images = get_images_from_url(url)
        
        if not images:
            print(f"  âš ï¸  GÃ¶rsel bulunamadÄ±!")
            stats['no_images'] += 1
            # Excel'de boÅŸ bÄ±rak (zaten boÅŸ)
        else:
            print(f"  âœ… {len(images)} gÃ¶rsel bulundu")
            
            # Ä°lk gÃ¶rseli "Ana gÃ¶rsel" sÃ¼tununa yaz
            if len(images) > 0:
                df.at[index, 'Ana gÃ¶rsel'] = images[0]
                print(f"    âœ“ Ana gÃ¶rsel: {images[0][:80]}...")
            
            # DiÄŸer gÃ¶rselleri "DiÄŸer gÃ¶rseller" sÃ¼tununa virgÃ¼lle ayÄ±rarak yaz
            if len(images) > 1:
                other_images = images[1:]
                df.at[index, 'DiÄŸer gÃ¶rseller'] = ', '.join(other_images)
                print(f"    âœ“ {len(other_images)} diÄŸer gÃ¶rsel eklendi")
            
            stats['success'] += 1
        
        # Her SAVE_INTERVAL Ã¼rÃ¼nde bir veya son Ã¼rÃ¼n ise Excel'i kaydet
        if idx % SAVE_INTERVAL == 0 or idx == total_products:
            print(f"\nğŸ’¾ Ä°lerleme kaydediliyor... ({idx}/{total_products})")
            df.to_excel(EXCEL_FILE, index=False)
            print(f"âœ… Excel dosyasÄ± gÃ¼ncellendi: {EXCEL_FILE}\n")
        
        # Rate limiting iÃ§in bekle
        time.sleep(DELAY)
        print()
    
    # Son bir kez daha kaydet (gÃ¼vence iÃ§in)
    print("\nExcel dosyasÄ± gÃ¼ncelleniyor...")
    df.to_excel(EXCEL_FILE, index=False)
    print(f"âœ… Excel dosyasÄ± gÃ¼ncellendi: {EXCEL_FILE}")
    
    # Ã–zet
    print("\n" + "="*60)
    print("Ã–ZET")
    print("="*60)
    print(f"BaÅŸarÄ±lÄ±: {stats['success']}")
    print(f"GÃ¶rsel bulunamayan: {stats['no_images']}")
    print(f"Toplam: {total_products}")

if __name__ == '__main__':
    main()

