#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TechnoMarket.bg grid sayfasÄ±ndan Ã¼rÃ¼n linklerini Ã§eken script
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time
import os

BASE_URL = 'https://www.technomarket.bg'
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
EXCEL_FILE = 'Product_URLs.xlsx'
DELAY = 1  # Her istek arasÄ±nda bekleme sÃ¼resi

def extract_product_urls(page_url):
    """Sayfadan Ã¼rÃ¼n linklerini Ã§Ä±karÄ±r - tm-product-item yapÄ±sÄ±ndan"""
    try:
        print(f"Sayfa Ã§ekiliyor: {page_url}")
        response = requests.get(page_url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        product_urls = []
        seen = set()
        
        # tm-product-item elementlerini bul
        product_items = soup.find_all('tm-product-item')
        
        if not product_items:
            # Alternatif: direkt product-image veya title class'lÄ± linkleri bul
            product_items = soup.find_all('a', class_=['product-image', 'title'])
        
        for item in product_items:
            # EÄŸer tm-product-item ise, iÃ§indeki linkleri bul
            if item.name == 'tm-product-item':
                links = item.find_all('a', href=True)
            else:
                links = [item] if item.get('href') else []
            
            for link in links:
                href = link.get('href', '')
                if not href:
                    continue
                
                # URL'yi normalize et
                if href.startswith('/'):
                    full_url = urljoin(BASE_URL, href)
                elif href.startswith('http') and 'technomarket.bg' in href:
                    full_url = href
                else:
                    continue
                
                # PDF linklerini at
                if '.pdf' in full_url.lower():
                    continue
                
                # ÃœrÃ¼n sayfasÄ± linklerini filtrele (kategori deÄŸil)
                # Ã–rnek: /televizor/neo-led-32h3m-hd-led-tv-09218598
                if '/produkti/' not in full_url and full_url not in seen:
                    seen.add(full_url)
                    product_urls.append(full_url)
        
        # TekrarlarÄ± temizle ve sÄ±rala
        unique_urls = list(dict.fromkeys(product_urls))  # SÄ±rayÄ± koruyarak tekrarlarÄ± kaldÄ±r
        
        print(f"  âœ… {len(unique_urls)} Ã¼rÃ¼n linki bulundu")
        return unique_urls
    
    except Exception as e:
        print(f"  âœ— Hata: {str(e)}")
        return []

def main():
    print("TechnoMarket.bg ÃœrÃ¼n Link Ã‡ekici")
    print("="*60)
    
    # Mevcut Excel dosyasÄ±nÄ± oku (varsa)
    existing_urls = []
    try:
        if os.path.exists(EXCEL_FILE):
            print(f"\nğŸ“‚ Mevcut Excel dosyasÄ± bulundu: {EXCEL_FILE}")
            existing_df = pd.read_excel(EXCEL_FILE)
            if 'Product URL' in existing_df.columns:
                existing_urls = existing_df['Product URL'].dropna().astype(str).tolist()
                existing_urls = [url.strip() for url in existing_urls if url.strip()]
                print(f"  âœ… {len(existing_urls)} mevcut Ã¼rÃ¼n linki yÃ¼klendi")
            else:
                print("  âš ï¸  'Product URL' sÃ¼tunu bulunamadÄ±, yeni dosya oluÅŸturulacak")
    except Exception as e:
        print(f"  âš ï¸  Mevcut dosya okunamadÄ±: {str(e)}")
        print("  Yeni dosya oluÅŸturulacak")
    
    # KullanÄ±cÄ±dan toplu URL listesi al
    print("\nGrid sayfalarÄ±nÄ±n URL'lerini girin (her satÄ±ra bir URL, boÅŸ satÄ±r ile bitirin):")
    print("(Ã–rnek: https://www.technomarket.bg/produkti/televizor)")
    
    page_urls = []
    while True:
        url = input().strip()
        if not url:
            break
        if url:
            if not url.startswith('http'):
                url = urljoin(BASE_URL, url)
            page_urls.append(url)
    
    if not page_urls:
        print("âš ï¸  URL girilmedi!")
        return
    
    print(f"\nâœ… {len(page_urls)} kategori URL'si alÄ±ndÄ±")
    print("-" * 60)
    
    # ÃœrÃ¼n linklerini Ã§ek (mevcut URL'leri de dahil et)
    all_urls = list(existing_urls)  # Mevcut URL'leri baÅŸlangÄ±Ã§ listesine ekle
    
    # Her kategori URL'si iÃ§in iÅŸlem yap
    for cat_idx, page_url in enumerate(page_urls, 1):
        print(f"\n[{cat_idx}/{len(page_urls)}] Ä°ÅŸlenen kategori: {page_url}")
        print("-" * 60)
        
        # Ä°lk sayfadan Ã¼rÃ¼nleri Ã§ek
        urls = extract_product_urls(page_url)
        all_urls.extend(urls)
        
        # Sayfalama varsa diÄŸer sayfalarÄ± da Ã§ek
        print("Sayfalama kontrol ediliyor...")
        page_num = 2
        max_pages = 100  # Maksimum sayfa sayÄ±sÄ±
        
        while page_num <= max_pages:
            # Sayfa URL'sini oluÅŸtur (technomarket.bg formatÄ±na gÃ¶re)
            # URL formatÄ±: /produkti/televizor?page=2
            if '?' in page_url:
                # Zaten parametre var, page ekle veya gÃ¼ncelle
                if 'page=' in page_url:
                    next_page_url = re.sub(r'page=\d+', f'page={page_num}', page_url)
                else:
                    next_page_url = f"{page_url}&page={page_num}"
            else:
                next_page_url = f"{page_url}?page={page_num}"
            
            time.sleep(DELAY)
            urls = extract_product_urls(next_page_url)
            
            if not urls:
                print(f"  Sayfa {page_num}'de Ã¼rÃ¼n bulunamadÄ±, sayfalama sona erdi.")
                break
            
            # Ã–nceki sayfalarda olan URL'ler varsa durdur
            new_urls = [u for u in urls if u not in all_urls]
            if not new_urls:
                print(f"  Sayfa {page_num}'de yeni Ã¼rÃ¼n yok, sayfalama sona erdi.")
                break
            
            all_urls.extend(new_urls)
            print(f"  Toplam {len(all_urls)} Ã¼rÃ¼n linki toplandÄ±")
            
            page_num += 1
        
        print(f"âœ… Kategori {cat_idx} tamamlandÄ±. Toplam {len(all_urls)} Ã¼rÃ¼n linki")
        time.sleep(DELAY)  # Kategoriler arasÄ± bekleme
    
    # TekrarlarÄ± temizle
    final_urls = list(dict.fromkeys(all_urls))
    
    # Yeni eklenen URL sayÄ±sÄ±nÄ± hesapla
    new_urls_count = len(final_urls) - len(existing_urls)
    
    print(f"\nâœ… Toplam {len(final_urls)} benzersiz Ã¼rÃ¼n linki bulundu")
    if existing_urls:
        print(f"   ({len(existing_urls)} mevcut + {new_urls_count} yeni)")
    
    # Excel'e kaydet
    print("\nğŸ’¾ Excel dosyasÄ±na kaydediliyor...")
    df = pd.DataFrame({
        'Product URL': final_urls
    })
    df.to_excel(EXCEL_FILE, index=False)
    print(f"âœ… ÃœrÃ¼n linkleri '{EXCEL_FILE}' dosyasÄ±na kaydedildi")
    if new_urls_count > 0:
        print(f"   (+{new_urls_count} yeni link eklendi)")
    
    # Ã–zet
    print("\n" + "="*60)
    print("Ã–ZET")
    print("="*60)
    print(f"Ä°ÅŸlenen kategori sayÄ±sÄ±: {len(page_urls)}")
    print(f"Toplam Ã¼rÃ¼n sayÄ±sÄ±: {len(final_urls)}")
    if existing_urls:
        print(f"  - Mevcut: {len(existing_urls)}")
        print(f"  - Yeni eklenen: {new_urls_count}")
    print(f"Excel dosyasÄ±: {EXCEL_FILE}")
    
    # Ä°lk 5 linki gÃ¶ster
    if final_urls:
        print("\nÄ°lk 5 Ã¼rÃ¼n linki:")
        for i, url in enumerate(final_urls[:5], 1):
            print(f"  {i}. {url}")

if __name__ == '__main__':
    main()

