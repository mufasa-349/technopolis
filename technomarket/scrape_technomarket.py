#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TechnoMarket.bg sitesinden Ã¼rÃ¼n bilgilerini Ã§eken script
Bulgarca metinleri TÃ¼rkÃ§e'ye Ã§evirir
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin, urlparse
import json
import re

# Ã‡eviri iÃ§in (deep-translator kullanÄ±lacak)
try:
    from deep_translator import GoogleTranslator
    TRANSLATOR_AVAILABLE = True
except ImportError:
    TRANSLATOR_AVAILABLE = False
    print("âš ï¸  deep-translator paketi bulunamadÄ±. Ã‡eviri yapÄ±lmayacak.")
    print("   YÃ¼klemek iÃ§in: pip install deep-translator")

# Ayarlar
BASE_URL = 'https://www.technomarket.bg'
EXCEL_FILE = 'TechnoMarket_Urunler.xlsx'
DELAY = 2  # Her istek arasÄ±nda bekleme sÃ¼resi (saniye)
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def translate_text(text, source='bg', target='tr'):
    """Bulgarca metni TÃ¼rkÃ§e'ye Ã§evirir"""
    if not TRANSLATOR_AVAILABLE or not text or pd.isna(text):
        return text
    
    try:
        text_str = str(text).strip()
        if not text_str:
            return text
        
        translator = GoogleTranslator(source=source, target=target)
        translated = translator.translate(text_str)
        time.sleep(0.5)  # Rate limiting
        return translated
    except Exception as e:
        print(f"    Ã‡eviri hatasÄ±: {str(e)}")
        return text

def extract_price(price_text):
    """Fiyat metninden sayÄ±sal deÄŸeri Ã§Ä±karÄ±r (tam sayÄ± olarak)"""
    if not price_text:
        return None
    
    # SayÄ±larÄ± ve noktayÄ± bul
    price_str = re.sub(r'[^\d.,]', '', str(price_text))
    price_str = price_str.replace(',', '.').replace(' ', '')
    
    try:
        # VirgÃ¼lden sonraki kÄ±smÄ± at, tam sayÄ± dÃ¶ndÃ¼r
        price_float = float(price_str)
        return int(price_float)
    except:
        return None

def get_product_details(product_url, timeout=3):
    """ÃœrÃ¼n sayfasÄ±ndan detaylarÄ± Ã§eker"""
    try:
        response = requests.get(product_url, headers=HEADERS, timeout=timeout)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        product_data = {
            'product_id': None,
            'product_name': None,
            'price': None,
            'category': None,
            'brand': None,
            'description': None,
            'images': [],
            'ean': None
        }
        
        # ÃœrÃ¼n adÄ± - Ã¶nce .name span'Ä±ndan al
        name_elem = soup.select_one('.name')
        if name_elem:
            product_data['product_name'] = name_elem.get_text(strip=True)
        else:
            # Alternatif seÃ§iciler
            name_selectors = [
                'h1.product-title',
                'h1.product-name',
                '.product-title',
                'h1',
                '[class*="product"][class*="name"]',
                '[class*="product"][class*="title"]'
            ]
            for selector in name_selectors:
                name_elem = soup.select_one(selector)
                if name_elem:
                    product_data['product_name'] = name_elem.get_text(strip=True)
                    break
        
        # Fiyat - TechnoMarket Ã¶zel yapÄ±sÄ±
        # <div class="price"><tm-price> iÃ§inde <span class="bgn"><span class="primary">1,099</span><span class="secondary">00</span></span>
        price_elem = soup.select_one('.price tm-price .bgn')
        if price_elem:
            # primary ve secondary span'larÄ±nÄ± bul
            primary = price_elem.select_one('.primary')
            secondary = price_elem.select_one('.secondary')
            
            if primary:
                primary_text = primary.get_text(strip=True)
                secondary_text = secondary.get_text(strip=True) if secondary else '00'
                
                # Binlik ayÄ±racÄ± (virgÃ¼l) ve boÅŸluklarÄ± temizle
                primary_text = primary_text.replace(',', '').replace(' ', '').strip()
                secondary_text = secondary_text.replace(',', '').replace(' ', '').strip()
                
                # FiyatÄ± birleÅŸtir (1099.00 gibi)
                price_text = f"{primary_text}.{secondary_text}"
                product_data['price'] = extract_price(price_text)
        
        # EÄŸer yukarÄ±daki yapÄ± Ã§alÄ±ÅŸmadÄ±ysa, genel yÃ¶ntemi dene
        if not product_data['price']:
            price_selectors = [
                '.price',
                '.product-price',
                '[class*="price"]',
                '[data-price]'
            ]
            for selector in price_selectors:
                price_elem = soup.select_one(selector)
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    product_data['price'] = extract_price(price_text)
                    if product_data['price']:
                        break
        
        # EAN/Barkod - tm-pointandplace elementinin ean attribute'Ã¼nden al
        pointandplace = soup.select_one('tm-pointandplace')
        if pointandplace:
            ean = pointandplace.get('ean')
            if ean:
                product_data['ean'] = ean.strip()
        
        # EÄŸer tm-pointandplace'den bulunamadÄ±ysa, alternatif yÃ¶ntemleri dene
        if not product_data['ean']:
            ean_patterns = [
                r'EAN[:\s]*(\d+)',
                r'Barkod[:\s]*(\d+)',
                r'ĞšĞ¾Ğ´ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ°[:\s]*(\d+)'
            ]
            for pattern in ean_patterns:
                match = re.search(pattern, soup.get_text(), re.IGNORECASE)
                if match:
                    product_data['ean'] = match.group(1)
                    break
        
        # ÃœrÃ¼n ID - Åu an EAN olarak bulunan deÄŸeri kullan (ĞšĞ¾Ğ´ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ°)
        # EÄŸer bulunamazsa URL'den al
        product_code_patterns = [
            r'ĞšĞ¾Ğ´ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ°[:\s]*(\d+)',
            r'ĞšĞ¾Ğ´[:\s]*(\d+)',
        ]
        for pattern in product_code_patterns:
            match = re.search(pattern, soup.get_text(), re.IGNORECASE)
            if match:
                product_data['product_id'] = match.group(1)
                break
        
        # EÄŸer hala bulunamadÄ±ysa URL'den al
        if not product_data['product_id']:
            url_match = re.search(r'/p/(\d+)|/product/(\d+)|product-(\d+)|/(\d{8})$', product_url)
            if url_match:
                product_data['product_id'] = url_match.group(1) or url_match.group(2) or url_match.group(3) or url_match.group(4)
        
        # Marka - Ã¶nce data-brand attribute'Ã¼nden al (Ã§eviri yok)
        brand_elem = soup.select_one('[data-brand]')
        if brand_elem:
            product_data['brand'] = brand_elem.get('data-brand', '').strip()
        
        # EÄŸer data-brand bulunamadÄ±ysa, diÄŸer yÃ¶ntemleri dene
        if not product_data['brand']:
            brand_selectors = [
                '[class*="brand"]',
                '.product-brand'
            ]
            for selector in brand_selectors:
                brand_elem = soup.select_one(selector)
                if brand_elem:
                    product_data['brand'] = brand_elem.get_text(strip=True)
                    break
        
        # Kategori - Ã¶nce data-category attribute'Ã¼nden al
        category_elem = soup.select_one('[data-category]')
        if category_elem:
            category_value = category_elem.get('data-category', '').strip()
            if category_value:
                # Category formatÄ±: "Ğ¢Ğ’, ĞÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ•Ğ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ¸ĞºĞ°|Ğ¢ĞµĞ»ĞµĞ²Ğ¸Ğ·Ğ¾Ñ€Ğ¸|32 "_ 42 ""
                # TÃ¼m kategori hiyerarÅŸisini " > " ile birleÅŸtir
                category_parts = [part.strip() for part in category_value.split('|') if part.strip()]
                if category_parts:
                    # TÃ¼m kategorileri " > " ile birleÅŸtir
                    product_data['category'] = ' > '.join(category_parts)
                else:
                    product_data['category'] = category_value
        
        # EÄŸer data-category bulunamadÄ±ysa, alternatif yÃ¶ntemleri dene
        if not product_data['category']:
            category_selectors = [
                '.breadcrumb a',
                '[class*="breadcrumb"] a',
                '[class*="category"]'
            ]
            for selector in category_selectors:
                cat_elems = soup.select(selector)
                if cat_elems:
                    product_data['category'] = cat_elems[-1].get_text(strip=True)
                    break
        
        # AÃ§Ä±klama - .collapsed-content .product-basic ul li elementlerinden
        desc_items = []
        
        # Ã–nce .collapsed-content .product-basic ul li'den Ã§ek
        basic_info = soup.select('.collapsed-content .product-basic ul li')
        if basic_info:
            for li in basic_info:
                text = li.get_text(strip=True)
                if text:
                    # Ä°kon metnini temizle (âœ“ iÅŸaretini kaldÄ±r)
                    text = re.sub(r'^[^\w]*', '', text)
                    if text:
                        desc_items.append(text)
        
        # EÄŸer bulunamadÄ±ysa alternatif yÃ¶ntemleri dene
        if not desc_items:
            desc_selectors = [
                '.product-description',
                '[class*="description"]',
                '.product-details'
            ]
            for selector in desc_selectors:
                desc_elem = soup.select_one(selector)
                if desc_elem:
                    desc_text = desc_elem.get_text(strip=True)
                    if desc_text:
                        desc_items.append(desc_text)
                        break
        
        # AÃ§Ä±klamalarÄ± birleÅŸtir (satÄ±r baÅŸÄ± ile)
        if desc_items:
            product_data['description'] = '\n'.join(desc_items)
        else:
            product_data['description'] = ''
        
        # GÃ¶rseller - .slider-content iÃ§inden Ã§ek
        slider_content = soup.select_one('.slider-content')
        if slider_content:
            imgs = slider_content.find_all('img')
            for img in imgs:
                # Ã–nce src, sonra data-src'yi kontrol et
                img_url = img.get('src') or img.get('data-src')
                if img_url:
                    # URL'yi normalize et
                    if img_url.startswith('//'):
                        img_url = f"https:{img_url}"
                    elif img_url.startswith('/'):
                        img_url = urljoin(BASE_URL, img_url)
                    
                    # TekrarlarÄ± Ã¶nle
                    if img_url and img_url not in product_data['images']:
                        product_data['images'].append(img_url)
        
        # EÄŸer slider-content'ten gÃ¶rsel bulunamadÄ±ysa, alternatif yÃ¶ntemleri dene
        if not product_data['images']:
            img_selectors = [
                '.product-gallery img',
                '.product-images img',
                '[class*="product"] img[src]',
                'img[data-zoom-url]',
                'img[data-large-url]'
            ]
            for selector in img_selectors:
                imgs = soup.select(selector)
                for img in imgs:
                    img_url = img.get('src') or img.get('data-src') or img.get('data-zoom-url') or img.get('data-large-url')
                    if img_url:
                        if img_url.startswith('//'):
                            img_url = f"https:{img_url}"
                        elif img_url.startswith('/'):
                            img_url = urljoin(BASE_URL, img_url)
                        if img_url not in product_data['images']:
                            product_data['images'].append(img_url)
                if product_data['images']:
                    break
        
        return product_data
    
    except Exception as e:
        print(f"  Hata: {str(e)}")
        return None

def get_category_products(category_url, max_products=100):
    """Kategori sayfasÄ±ndan Ã¼rÃ¼n URL'lerini Ã§Ä±karÄ±r"""
    try:
        products = []
        page = 1
        
        while len(products) < max_products:
            # Sayfa numarasÄ±nÄ± URL'ye ekle
            if '?' in category_url:
                page_url = f"{category_url}&page={page}"
            else:
                page_url = f"{category_url}?page={page}"
            
            response = requests.get(page_url, headers=HEADERS, timeout=10)
            
            if response.status_code != 200:
                break
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ÃœrÃ¼n linklerini bul
            product_links = soup.find_all('a', href=re.compile(r'/p/|/product/'))
            
            found_new = False
            for link in product_links:
                href = link.get('href', '')
                if href:
                    if href.startswith('/'):
                        full_url = urljoin(BASE_URL, href)
                    elif not href.startswith('http'):
                        continue
                    else:
                        full_url = href
                    
                    if full_url not in products:
                        products.append(full_url)
                        found_new = True
                        if len(products) >= max_products:
                            break
            
            if not found_new:
                break
            
            page += 1
            time.sleep(DELAY)
        
        return products[:max_products]
    
    except Exception as e:
        print(f"Hata: {str(e)}")
        return []

def create_excel_template():
    """Excel ÅŸablonunu oluÅŸturur"""
    columns = [
        'Product ID',
        'Barkod (EAN Number)',
        'Product Name',
        'Price',
        'Currency',
        'Category',
        'Brand',
        'Ana gÃ¶rsel',
        'Image 1',
        'Image 2',
        'Image 3',
        'Image 4',
        'Image 5',
        'DiÄŸer gÃ¶rseller',
        'Product URL'
    ]
    
    df = pd.DataFrame(columns=columns)
    df.to_excel(EXCEL_FILE, index=False)
    print(f"âœ… Excel ÅŸablonu oluÅŸturuldu: {EXCEL_FILE}")

def main():
    print("TechnoMarket.bg ÃœrÃ¼n Ã‡ekme Scripti")
    print("="*60)
    
    # Excel dosyasÄ±nÄ± oluÅŸtur
    create_excel_template()
    
    # KullanÄ±cÄ±dan kategori seÃ§imi
    print("\nKategori URL'lerini girin (her satÄ±ra bir URL, boÅŸ satÄ±r ile bitirin):")
    category_urls = []
    while True:
        url = input().strip()
        if not url:
            break
        if url:
            if not url.startswith('http'):
                url = urljoin(BASE_URL, url)
            category_urls.append(url)
    
    if not category_urls:
        print("âš ï¸  Kategori URL'i girilmedi!")
        return
    
    print(f"\nâœ… {len(category_urls)} kategori iÅŸlenecek")
    
    # Excel dosyasÄ±nÄ± oku
    df = pd.read_excel(EXCEL_FILE)
    
    # Ä°statistikler
    total_products = 0
    stats = {'success': 0, 'failed': 0}
    
    # Her kategori iÃ§in Ã¼rÃ¼nleri Ã§ek
    for cat_idx, category_url in enumerate(category_urls, 1):
        print(f"\n[{cat_idx}/{len(category_urls)}] Kategori iÅŸleniyor: {category_url}")
        
        # ÃœrÃ¼n URL'lerini al
        product_urls = get_category_products(category_url)
        print(f"  âœ… {len(product_urls)} Ã¼rÃ¼n bulundu")
        
        # Her Ã¼rÃ¼n iÃ§in detaylarÄ± Ã§ek
        for prod_idx, product_url in enumerate(product_urls, 1):
            print(f"  [{prod_idx}/{len(product_urls)}] ÃœrÃ¼n iÅŸleniyor...")
            print(f"    URL: {product_url}")
            
            # ÃœrÃ¼n detaylarÄ±nÄ± Ã§ek
            product_data = get_product_details(product_url)
            
            if not product_data:
                print(f"    âœ— ÃœrÃ¼n bilgileri Ã§ekilemedi")
                stats['failed'] += 1
                time.sleep(DELAY)
                continue
            
            # Yeni satÄ±r oluÅŸtur
            new_row = {
                'Product ID': product_data.get('product_id', ''),
                'Barkod (EAN Number)': product_data.get('ean', ''),
                'Product Name': translate_text(product_data.get('product_name', '')),
                'Price': product_data.get('price'),
                'Currency': 'BGN',
                'Category': translate_text(product_data.get('category', '')),
                'Brand': translate_text(product_data.get('brand', '')),
                'Product URL': product_url,
                'Ana gÃ¶rsel': product_data['images'][0] if product_data['images'] else '',
                'Image 1': product_data['images'][1] if len(product_data['images']) > 1 else '',
                'Image 2': product_data['images'][2] if len(product_data['images']) > 2 else '',
                'Image 3': product_data['images'][3] if len(product_data['images']) > 3 else '',
                'Image 4': product_data['images'][4] if len(product_data['images']) > 4 else '',
                'Image 5': product_data['images'][5] if len(product_data['images']) > 5 else '',
                'DiÄŸer gÃ¶rseller': ', '.join(product_data['images'][6:]) if len(product_data['images']) > 6 else ''
            }
            
            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
            total_products += 1
            stats['success'] += 1
            
            print(f"    âœ“ ÃœrÃ¼n eklendi: {new_row['Product Name'][:50]}...")
            
            # Her 10 Ã¼rÃ¼nde bir kaydet
            if total_products % 10 == 0:
                print(f"\nğŸ’¾ Ä°lerleme kaydediliyor... ({total_products} Ã¼rÃ¼n)")
                df.to_excel(EXCEL_FILE, index=False)
            
            time.sleep(DELAY)
    
    # Son kayÄ±t
    print("\nğŸ’¾ Excel dosyasÄ± gÃ¼ncelleniyor...")
    df.to_excel(EXCEL_FILE, index=False)
    print(f"âœ… Excel dosyasÄ± gÃ¼ncellendi: {EXCEL_FILE}")
    
    # Ã–zet
    print("\n" + "="*60)
    print("Ã–ZET")
    print("="*60)
    print(f"BaÅŸarÄ±lÄ±: {stats['success']}")
    print(f"BaÅŸarÄ±sÄ±z: {stats['failed']}")
    print(f"Toplam: {total_products}")

if __name__ == '__main__':
    main()

