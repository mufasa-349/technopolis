#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TechnoMarket.bg grid sayfasÄ±ndan Ã¼rÃ¼n linklerini Ã§eken script
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time

BASE_URL = 'https://www.technomarket.bg'
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
EXCEL_FILE = 'Product_URLs.xlsx'
DELAY = 1  # Her istek arasÄ±nda bekleme sÃ¼resi

def extract_product_urls(page_url):
    """Sayfadan Ã¼rÃ¼n linklerini Ã§Ä±karÄ±r - tm-product-item yapÄ±sÄ±ndan"""
    try:
        print(f"Sayfa Ã§ekiliyor: {page_url}")
        response = requests.get(page_url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        product_urls = []
        seen = set()
        
        # tm-product-item elementlerini bul
        product_items = soup.find_all('tm-product-item')
        
        if not product_items:
            # Alternatif: direkt product-image veya title class'lÄ± linkleri bul
            product_items = soup.find_all('a', class_=['product-image', 'title'])
        
        for item in product_items:
            # EÄŸer tm-product-item ise, iÃ§indeki linkleri bul
            if item.name == 'tm-product-item':
                links = item.find_all('a', href=True)
            else:
                links = [item] if item.get('href') else []
            
            for link in links:
                href = link.get('href', '')
                if not href:
                    continue
                
                # URL'yi normalize et
                if href.startswith('/'):
                    full_url = urljoin(BASE_URL, href)
                elif href.startswith('http') and 'technomarket.bg' in href:
                    full_url = href
                else:
                    continue
                
                # ÃœrÃ¼n sayfasÄ± linklerini filtrele (kategori deÄŸil)
                # Ã–rnek: /televizor/neo-led-32h3m-hd-led-tv-09218598
                if '/produkti/' not in full_url and full_url not in seen:
                    seen.add(full_url)
                    product_urls.append(full_url)
        
        # TekrarlarÄ± temizle ve sÄ±rala
        unique_urls = list(dict.fromkeys(product_urls))  # SÄ±rayÄ± koruyarak tekrarlarÄ± kaldÄ±r
        
        print(f"  âœ… {len(unique_urls)} Ã¼rÃ¼n linki bulundu")
        return unique_urls
    
    except Exception as e:
        print(f"  âœ— Hata: {str(e)}")
        return []

def main():
    print("TechnoMarket.bg ÃœrÃ¼n Link Ã‡ekici")
    print("="*60)
    
    # KullanÄ±cÄ±dan URL al
    print("\nGrid sayfasÄ±nÄ±n URL'sini girin:")
    print("(Ã–rnek: https://www.technomarket.bg/produkti/televizor)")
    page_url = input().strip()
    
    if not page_url:
        print("âš ï¸  URL girilmedi!")
        return
    
    if not page_url.startswith('http'):
        page_url = urljoin(BASE_URL, page_url)
    
    print(f"\nÄ°ÅŸlenen URL: {page_url}")
    print("-" * 60)
    
    # ÃœrÃ¼n linklerini Ã§ek
    all_urls = []
    
    # Ä°lk sayfadan Ã¼rÃ¼nleri Ã§ek
    urls = extract_product_urls(page_url)
    all_urls.extend(urls)
    
    # Sayfalama varsa diÄŸer sayfalarÄ± da Ã§ek
    print("\nSayfalama kontrol ediliyor...")
    page_num = 2
    max_pages = 100  # Maksimum sayfa sayÄ±sÄ±
    
    while page_num <= max_pages:
        # Sayfa URL'sini oluÅŸtur (technomarket.bg formatÄ±na gÃ¶re)
        # URL formatÄ±: /produkti/televizor?page=2
        if '?' in page_url:
            # Zaten parametre var, page ekle veya gÃ¼ncelle
            if 'page=' in page_url:
                next_page_url = re.sub(r'page=\d+', f'page={page_num}', page_url)
            else:
                next_page_url = f"{page_url}&page={page_num}"
        else:
            next_page_url = f"{page_url}?page={page_num}"
        
        time.sleep(DELAY)
        urls = extract_product_urls(next_page_url)
        
        if not urls:
            print(f"  Sayfa {page_num}'de Ã¼rÃ¼n bulunamadÄ±, sayfalama sona erdi.")
            break
        
        # Ã–nceki sayfalarda olan URL'ler varsa durdur
        new_urls = [u for u in urls if u not in all_urls]
        if not new_urls:
            print(f"  Sayfa {page_num}'de yeni Ã¼rÃ¼n yok, sayfalama sona erdi.")
            break
        
        all_urls.extend(new_urls)
        print(f"  Toplam {len(all_urls)} Ã¼rÃ¼n linki toplandÄ±")
        
        page_num += 1
    
    # TekrarlarÄ± temizle
    final_urls = list(dict.fromkeys(all_urls))
    
    print(f"\nâœ… Toplam {len(final_urls)} benzersiz Ã¼rÃ¼n linki bulundu")
    
    # Excel'e kaydet
    print("\nğŸ’¾ Excel dosyasÄ±na kaydediliyor...")
    df = pd.DataFrame({
        'Product URL': final_urls
    })
    df.to_excel(EXCEL_FILE, index=False)
    print(f"âœ… ÃœrÃ¼n linkleri '{EXCEL_FILE}' dosyasÄ±na kaydedildi")
    
    # Ã–zet
    print("\n" + "="*60)
    print("Ã–ZET")
    print("="*60)
    print(f"Toplam Ã¼rÃ¼n sayÄ±sÄ±: {len(final_urls)}")
    print(f"Excel dosyasÄ±: {EXCEL_FILE}")
    
    # Ä°lk 5 linki gÃ¶ster
    if final_urls:
        print("\nÄ°lk 5 Ã¼rÃ¼n linki:")
        for i, url in enumerate(final_urls[:5], 1):
            print(f"  {i}. {url}")

if __name__ == '__main__':
    main()

